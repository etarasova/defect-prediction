{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('input/bugzilla.csv')\n",
    "X = dataset.iloc[:, 2 :-1].values # independent variables\n",
    "y = dataset.iloc[:, -1].values # dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting features #1\n",
    "# Removing features with low variance\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(.85 * (1 - .85)))\n",
    "X = sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "# Selecting features #2\n",
    "# Removing high-correlated columns\n",
    "\n",
    "# Convert X into DataFrame\n",
    "df = pd.DataFrame(X)\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find features with correlation greater than 0.8\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "print(to_drop)\n",
    "\n",
    "# Drop features \n",
    "df.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Convert dataframe back to X\n",
    "X = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4620, 11)\n",
      "(4620, 10)\n"
     ]
    }
   ],
   "source": [
    "# Selecting features #3\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "print(X.shape)\n",
    "X_new = SelectKBest(chi2, k=10).fit_transform(X, y)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer()\n",
    "X_train = pt.fit_transform(X_train)\n",
    "X_test = pt.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000, random_state=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Logistic Regression model on the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0, max_iter=10000)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "#print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[503  89]\n",
      " [161 171]]\n",
      "Accuracy: 72.943723 %\n",
      "Precision: 65.769231 %\n",
      "Recall: 51.506024 %\n",
      "F1 score: 57.770270 %\n"
     ]
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(\"Accuracy: {:2f} %\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision: {:2f} %\".format(precision_score(y_test, y_pred)*100))\n",
    "print(\"Recall: {:2f} %\".format(recall_score(y_test, y_pred)*100))\n",
    "print(\"F1 score: {:2f} %\".format(f1_score(y_test, y_pred)*100))\n",
    "\n",
    "# shows how many correct and incorrect predictions we made (0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_accuracy: 71.635402 %\n",
      "Train_recall_macro: 64.655812 %\n",
      "Train_precision_macro: 71.412749 %\n",
      "Train_f1_macro: 65.032426 %\n",
      "Train_roc_auc: 75.393416 %\n"
     ]
    }
   ],
   "source": [
    "# Computing the accuracy with cross-validate\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn import svm\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "\n",
    "scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro', 'roc_auc']\n",
    "accuracies = cross_validate(estimator = classifier, X = X, y = y, cv = 10, scoring=scoring, return_train_score=True)\n",
    "#print(sorted(accuracies.keys()))\n",
    "print(\"Train_accuracy: {:2f} %\".format(accuracies['train_accuracy'].mean()*100))\n",
    "print(\"Train_recall_macro: {:2f} %\".format(accuracies['train_recall_macro'].mean()*100))\n",
    "print(\"Train_precision_macro: {:2f} %\".format(accuracies['train_precision_macro'].mean()*100))\n",
    "print(\"Train_f1_macro: {:2f} %\".format(accuracies['train_f1_macro'].mean()*100))\n",
    "print(\"Train_roc_auc: {:2f} %\".format(accuracies['train_roc_auc'].mean()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.580087 %\n",
      "Recall: 38.133386 %\n",
      "Precision: 71.144124 %\n",
      "F1: 49.562018 %\n",
      "Roc_auc: 64.565574 %\n"
     ]
    }
   ],
   "source": [
    "# Computing the accuracy with KFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "kf = KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "\n",
    "classifier = LogisticRegression(random_state = 0, max_iter=10000)\n",
    "\n",
    "acc_score = []\n",
    "rec_score = []\n",
    "prec_score = []\n",
    "f1 = []\n",
    "roc_auc = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train , X_test = X[train_index], X[test_index]\n",
    "    y_train , y_test = y[train_index] , y[test_index]\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    #y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    THRESHOLD = 0.5\n",
    "    \n",
    "    y_pred_new = np.where(classifier.predict_proba(X_test)[:,1] >= THRESHOLD, 1, 0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc_score.append(accuracy_score(y_test, y_pred_new)*100)\n",
    "    prec_score.append(precision_score(y_test, y_pred_new)*100)\n",
    "    rec_score.append(recall_score(y_test, y_pred_new)*100)\n",
    "    f1.append(f1_score(y_test, y_pred_new)*100)\n",
    "    roc_auc.append(roc_auc_score(y_test, y_pred_new)*100)\n",
    "\n",
    "\n",
    "avg_accuracy = (sum(acc_score)/10)\n",
    "print('Accuracy: {:2f} %'.format(avg_accuracy))\n",
    "\n",
    "avg_recall = (sum(rec_score)/10)\n",
    "print('Recall: {:2f} %'.format(avg_recall))\n",
    "\n",
    "avg_precision = (sum(prec_score)/10)\n",
    "print('Precision: {:2f} %'.format(avg_precision))\n",
    "\n",
    "avg_f1 = (sum(f1)/10)\n",
    "print('F1: {:2f} %'.format(avg_f1))\n",
    "\n",
    "avg_roc_auc = (sum(roc_auc)/10)\n",
    "print('Roc_auc: {:2f} %'.format(avg_roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.949490 %\n",
      "Recall: 38.906498 %\n",
      "Precision: 70.739195 %\n",
      "F1: 49.437622 %\n",
      "Roc_auc: 75.005684 %\n"
     ]
    }
   ],
   "source": [
    "# Computing the accuracy with cross_val_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "\n",
    "accuracy = cross_val_score(classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "print('Accuracy: {:2f} %'.format(accuracy.mean()*100))\n",
    "\n",
    "recall = cross_val_score(classifier, X_train, y_train, cv=kf, scoring='recall')\n",
    "print(\"Recall: {:2f} %\".format(recall.mean()*100))\n",
    "\n",
    "precision = cross_val_score(classifier, X_train, y_train, cv=kf, scoring='precision')\n",
    "print(\"Precision: {:2f} %\".format(precision.mean()*100))\n",
    "\n",
    "f1 = cross_val_score(classifier, X_train, y_train, cv=kf, scoring='f1')\n",
    "print(\"F1: {:2f} %\".format(f1.mean()*100))\n",
    "\n",
    "roc_auc = cross_val_score(classifier, X_train, y_train, cv=kf, scoring='roc_auc')\n",
    "print(\"Roc_auc: {:2f} %\".format(roc_auc.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
